\documentclass[a4paper]{article}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{times}
\usepackage{amssymb}
\usepackage{mathtools}	% pulls in amsmath
	\mathtoolsset{centercolon}
\usepackage{tikz}
	\usetikzlibrary{automata}
\usepackage{mathpartir}
\usepackage{amsthm}
\usepackage{amsxtra}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{semantic}
	\reservestyle{\declarevars}{\texttt}
	\reservestyle{\declareops}{\texttt}
	\reservestyle{\declarestates}{\text}
\usepackage{color}
\usepackage{listings}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}

\newtheorem{myexample}{\textbf{Example}}
\newtheorem{mylemma}{\textbf{Lemma}}
\newtheorem{myproof}{\textbf{Proof}}
\newtheorem{myinvariant}{\textbf{Invariant}}
\newtheorem{mytheorem}{\textbf{Theorem}}
\newtheorem{mycorollary}{\textbf{Corollary}}
\newtheorem{myapproach}{Approach}
\newtheorem{myproperty}{Property}
\newtheorem{mydefinition}{Definition}

\newtheorem{mycase}{Case}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\small,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
 % frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  columns=fullflexible,	% not monospace
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  morekeywords={then, end, and, or, assign, increment, decrement, jump, jump_if, store, *, +},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                       % sets default tabsize to 2 spaces
  title=\lstname,                  % show the filename of files included with \lstinputlisting; also try caption instead of title
  mathescape
}

\newcommand*{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\newcommand{\any}{{\rule[-.2ex]{1ex}{.4pt}}}	% Less hideous than \_.
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RP}{\RR_{\ge 0}}
\newcommand*{\dave}[1]{{\color{red}\textbf{PDS: #1}}}
\newcommand{\ie}{\emph{i.e.,} }
\newcommand{\eg}{\emph{e.g.,} }
\usepackage{hyperref}
\newcommand*{\figref}[1]{\hyperref[#1]{Figure~\ref*{#1}}}

\title{Exercise Sheet 1---Algorithms and Data Structures}
\author{Felipe Cerqueira \\ 2547787 \and David Swasey \\ 2542105}

\begin{document}

\maketitle

Tutorial time: Monday 14:00

\section{Exercise 1 (10 pts)}

\paragraph{a)} Prove the statement from the lecture: If for two functions $f, g: \NN \to \RP$, the value $L := \lim_{n \to \infty} \frac{f(n)}{g(n)} \in [0, \infty)$ exists, then

\begin{itemize}
\item if $L = 0$, then $f(n) \in o(g(n))$.
\item if $L \in (0, \infty)$, then $f(n) \in \Theta(g(n))$.
\item if $L = \infty$, then $f(n) \in \omega(g(n))$.
\end{itemize}

\paragraph{Answer:}

We use the definition of limit.
(The definitions for O-notation assume that $f$ and $g$ are positive functions.
We modified the problem statement accordingly.)

\begin{itemize}
\item \textbf{If $L = 0$, then $f(n) \in o(g(n))$.}

By definition of limit at infinity:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0, \left | \frac{f(n)}{g(n)} - L \right | < c$$

Since $L=0$ and the functions are positive, this implies that:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0, f(n) \le c \cdot g(n)$$

Thus, $f(n) = o(g(n))$.

\bigskip

\item \textbf{if $L \in (0, \infty)$, then $f(n) \in \Theta(g(n))$.}

By definition of limit at infinity:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0, \left | \frac{f(n)}{g(n)} - L \right | < c$$

This is equivalent to:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0,  \left | \frac{f(n) - L \cdot g(n)}{g(n)} \right | < c$$

Solving the inequality with the absolute value produces two solutions. First:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0,  f(n) \le (c + L) \cdot g(n)$$


Therefore, $\exists c' = (c + L) > 0, \exists n_0 \in \mathbb{N}, \forall n \ge n_0$, $f(n) \le c' g(n)$.
That is, $f(n) = O(g(n))$.

\bigskip And as the second solution we have:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0,  f(n) \ge (L - c) \cdot g(n)$$

Here $\exists c', 0 < c' < L, \exists n_0 \in \mathbb{N}, \forall n \ge n_0$, $f(n) \ge c' g(n)$. That is, $f(n) = \Omega(g(n))$.

\item \textbf{if $L = \infty$, then $f(n) \in \omega(g(n))$.}

By the definition of infinite limits at infinity,

$$\forall c > 0, \exists n_0>0, \forall n > n_0, \left | \frac{f(x)}{g(x)} \right | > c$$

That is,

$$\forall c > 0, \exists n_0>0, \forall n > n_0, f(x) > c \cdot g(x)$$

This is exactly the definition of $f(n) = \omega(g(n))$.


\end{itemize}

\paragraph{b)} Give an example of two (non-negative) functions $f,g$ with $f \in \Theta(g)$, but $\lim_{n \rightarrow \infty}\frac{f(n)}{g(n)}$ does not exist. 

\paragraph{Answer:}

Consider $f(n) = sin(n) + 1$ and $g(n) = cos(n) + 1$, which are both non-negative. Because the image of both functions lies in $[0, 2]$, they differ at most by some constant, so it must be that $f(n) = \Theta(g(n))$.
Now let's calculate the limit.

$$\lim_{n \rightarrow \infty} \frac{\sin(n) + 1}{\cos(n) + 1} = 
\lim_{n \rightarrow \infty} \frac{\sin(n)}{\cos(n) + 1} + \lim_{n \rightarrow \infty} \frac{1}{\cos(n) + 1}$$

By L'Hospital's Rule:

 
$$= \lim_{n \rightarrow \infty} \left ( \frac{\cos(n)}{-\sin(n)} + \frac{1}{-\sin(n)} \right )= \lim_{n \rightarrow \infty} \left (- \cot (n) - \csc (n) \right )$$

But since this function is periodic, the limit at infinity does not exist.

\paragraph{c)} Prove the following rules for O-notation:
\paragraph{Answer:}
\begin{itemize}
\item \textbf{For any positive constant} $c$, $c \cdot f(n) = \Theta(f(n))$.

Let $c$ be any positive constant. Note that $\forall n, c f(n) \le c f(n)$. Then $\exists c' = c > 0$ and $\exists n_0 = 0$, such that $\forall n \ge n_0$, $c\cdot f(n) \le c' \cdot f(n)$. So $c f(n) = O(f(n))$.

Likewise, $\forall n, c f(n) \ge c f(n)$. Then $\exists c' = c > 0$ and $\exists n_0 = 0$, such that $\forall n \ge n_0$, $c\cdot f(n) \ge c' \cdot f(n)$. So $c f(n) = \Omega(f(n))$.

\item $\mathbf{f(n) + g(n) = \Omega(f(n))}$.

Since $g(n) \ge 0$, then $f(n) + g(n) \ge 1 \cdot f(n)$. Thus, $\exists c = 1 > 0$ and $\exists n_0 = 0$, such that $\forall n \ge n_0, g(n) + f(n) \ge c \cdot f(n)$.

\item If $\mathbf{g(n) = O(f(n))}$, then $\mathbf{f(n) + g(n) = O(f(n))}$.

Assume that $\exists c > 0$ and $\exists n_0 \in 
\mathbb{N}$, such that $\forall n \ge n_0$, $g(n) \le c \cdot f(n)$.
Then, $\forall n \ge n_0$, $f(n) + g(n) \le f(n) + c \cdot f(n) = (c+1) \cdot f(n)$.

Therefore, $\exists c' = (c+1)$ and $\exists n_0 \in 
\mathbb{N}$, such that $\forall n \ge n_0$, $f(n) + g(n) \le c' \cdot f(n)$. That is, $f(n) + g(n) = O(f(n))$.


\item $O(f(n)) \cdot O(g(n)) = O(f(n) \cdot g(n))$.

Consider any $y(n) = O(f(n))$ and any $y' = O(g(n))$.

We know that $\exists c > 0$ and $\exists n_0 \in 
\mathbb{N}$, such that $\forall n \ge n_0$, $y(n) \le c \cdot f(n)$.

Similarly, $\exists c' > 0$ and $\exists n_0' \in 
\mathbb{N}$, such that $\forall n \ge n_0'$, $y'(n) \le c' \cdot g(n)$.

Let $n_0'' = n_0 + n_0'$.

Then $\forall n \ge n_0''$,

$$y(n) \le c \cdot f(n)$$

$$y'(n) \le c' \cdot g(n)$$

That is,

$$y(n) \cdot y'(n) \le c \cdot c' \cdot f(n) \cdot g(n)$$

Therefore, $\exists c'' = c \cdot c' > 0$ and $\exists n_0'' \in 
\mathbb{N}$, such that $\forall n \ge n_0''$, $y(n)\cdot y'(n) \le c'' \cdot f(n) \cdot g(n)$. It follows that $O(f(n)) \cdot O(g(n)) = O(f(n) \cdot g(n))$.

\end{itemize}

\paragraph{d)} Show that $a^n = o(b^n)$ and $\log_a n = \Theta (\log_b n)$ for any $1 < a < b$.

\paragraph{Answer:}

For the first case, note that $\lim_{n \rightarrow \infty}\frac{a^n}{b^n} = \lim_{n \rightarrow \infty}\left (\frac{a}{b} \right )^n = 0$, since $a/b < 1$. Thus, by the theorem from the lecture, $a^n = o(b^n)$.

For the other function:

$$\lim_{n \rightarrow \infty}\frac{\log_a n}{\log_b n} = \lim_{n \rightarrow \infty}\frac{\log_b n}{\log_b a \cdot \log_b n} = \lim_{n \rightarrow \infty} \log_a b \in (0,\infty)$$

By the theorem from the lecture, $\log_a n = \Theta(\log_b n)$.

\section{Exercise 2 (10 pts)}
\newcommand{\prob}{\operatorname{prob}}%

\paragraph{a)} Let $I$ and $J$ be two independent indicator variables with $\prob(X = 1) = p$ and $\prob(Y = 1) = q$, $p, q \in [0,1]$. Let $X = ( I + J) \mod 2$, that is, $X$ is one if and only if $I$ and $J$ are different. For which values of $p$ and $q$ are $I$ and $X$ independent? For which values are $I$, $J$, and $X$ independent?

\paragraph{Answer:}

Since $I$ and $J$ are independent, we can make a table with the events and probabilities.

\begin{center}
\begin{tabular}{ c | c | c | c }
  I = i & J = j & X = x & $\prob(I = i \wedge J = j \wedge X = x)$\\
  \hline                       
  0 & 0 & 0 & $(1-p)\cdot (1-q)$\\
  0 & 1 & 1 & $(1-p)\cdot q$\\
  1 & 0 & 1 & $p\cdot (1-q)$\\
  1 & 1 & 0 & $p \cdot q$\\
  \hline  
\end{tabular}
\end{center}

To enforce independence of $I$ and $X$, we must guarantee that:

$$ \forall i, x \in \{0,1\}, \prob(I = i \wedge X = x) = \prob(I = i) \cdot \prob(X = 1) $$
Expanding the quantifiers:
\begin{align*}
\prob(I = 0 \wedge X = 0) &= \prob(I = 0) \cdot \prob(X = 0) \land{} \\
\prob(I = 0 \wedge X = 1) &= \prob(I = 0) \cdot \prob(X = 1) \land{} \\
\prob(I = 1 \wedge X = 0) &= \prob(I = 1) \cdot \prob(X = 0) \land{} \\
\prob(I = 1 \wedge X = 1) &= \prob(I = 1) \cdot \prob(X = 1)
\end{align*}

Solving this set of equations, we obtain $p = 0$ and $q = 1/2$.

\bigskip
To enforce independence of $I$, $J$ and $X$, we must guarantee that:

$$ \forall i, j, x \in \{0,1\}, \prob(I = i \wedge J = j \wedge X = x) = \prob(I = i) \cdot \prob(J = j) \cdot \prob(X = 1) $$

However, this include the constraints for the impossible events:
\begin{align*}
0 = \prob(I = 0 \wedge J = 0 \wedge X = 1) = \prob(I = 0) \cdot \prob(J = 0) \cdot \prob(X = 1) \\
0 = \prob(I = 0 \wedge J = 1 \wedge X = 0) = \prob(I = 0) \cdot \prob(J = 1) \cdot \prob(X = 0) \\
0 = \prob(I = 1 \wedge J = 0 \wedge X = 0) = \prob(I = 1) \cdot \prob(J = 0) \cdot \prob(X = 0) \\
0 = \prob(I = 1 \wedge J = 1 \wedge X = 1) = \prob(I = 1) \cdot \prob(J = 1) \cdot \prob(X = 1)
\end{align*}

Because we know that each of the three variables can assume all values in $\{0, 1\}$ (\ie, $\prob(I = i), \prob(J = j), \prob(X = 1) > 0$), these equations cannot hold. Therefore, $I$, $J$, and $X$ cannot be made independent simultaneously.

\paragraph{b)} Let $X_1, \ldots, X_n$ be independent indicator variables with $\prob(X_i = 1) = p$ for all $i$. Show that $X = X_1 + \cdots + X_n$ is \emph{binomially distributed}, that is

$$\prob(X = k) = {n \choose k} p^k (1-p)^{n-k}.$$

\paragraph{Answer:}

\begin{proof}
By induction on the number of variables $n$. Let $X^{(i)} = X_1 + \cdots + X_i$. For the base case with a single variable $X_1$, we have:

\begin{align*}
\prob(X^{(1)} = 0) = \prob(X_1 = 0) = (1 - p) = {1 \choose 0} p^0 (1-p)^{1} \\
\prob(X^{(1)} = 1) = \prob(X_1 = 1) = p = {1 \choose 1} p^1 (1-p)^{0}
\end{align*}

For our IH, assume that for $i<n$ and $X^{(i)} = X_1 + \cdots + X_i$, the following holds:

$$\prob(X^{(i)} = k) = {i \choose k} p^k (1-p)^{i-k}.$$

Let's now consider the variable $X^{(i+1)} = X^{(i)} + X_{i+1}$, which includes one more term in the sum. Because $X_{i+1}$ can only assume two mutually-exclusive values (0 or 1):

\begin{align*}
\prob(X^{(i+1)} = k) & = \prob(X^{(i)} = k-1 \wedge X_{i+1} = 1) + \prob(X^{(i)} = k \wedge X_{i+1} = 0) && \text{[$X$ and $X_{i+1}$ are independent]}\\
& = \prob(X^{(i)} = k-1) \cdot \prob(X_{i+1} = 1) + \prob(X^{(i)} = k) \cdot \prob(X_{i+1} = 0) && \text{[Initial assumption]} \\
& = \prob(X^{(i)} = k-1) \cdot p + \prob(X^{(i)} = k) \cdot (1 - p) && \text{[By IH]}\\
& = {i \choose k-1} p^{k-1} (1-p)^{i-k+1} \cdot p + {i \choose k} p^k (1-p)^{i-k} \cdot (1 - p)\\
& = {i \choose k-1} p^{k} (1-p)^{i-k+1} + {i \choose k} p^k (1-p)^{i-k+1} \\
& = \left [ {i \choose k-1} + {i \choose k} \right] p^{k} (1-p)^{i-k+1}
&& \text{[By Pascal's rule]} \\
& =  {{i+1} \choose k} p^{k} (1-p)^{i-k+1}
\end{align*}

Therefore, the binomial property also holds when we include the extra term $X_{i+1}$. This completes the induction proof.
\end{proof}


\section{Exercise 3 (10 pts)}

Consider the following two alternatives to the list data structures as discussed in the lecture:

a) Every item has a pointer to its successor, but not its predecessor (singly-linked lists)

b) The list is doubly connected, but does not start with a dummy item. Instead, the head
pointer of the list points to the first element. Its predecessor and the successor of the last list element point to a special value \emph{NULL}.

\bigskip Discuss the advantages and disadvantages of these implementations, both in terms of asymptotic complexity and practical considerations.

\paragraph{Answer:}

Implementation (a) has the advantage of using less memory for the representation (only half the number of pointers).
Also, the operations are less error-prone as there are fewer pointer manipulations.
Operations at the head of the list continue to take constant time (\eg testing if the list is empty, consing on a new element, or removing the list head).
However, it makes some operations on the list more time-consuming (\eg adding an element at the end of the list, or concatenating two lists).
In general, any move/delete operation on some element $e$ requires updating the ``next-pointer'' from the element $e'$ that precedes $e$.
Therefore, these operations always need to be started from some predecessor of $e$, in order to identify this element $e'$.

Regarding implementation (b), there are no major drawbacks, except that it makes the list less modular and more susceptible to low-level programming issues. For example, deleting every element would require setting the $head$ pointer to \emph{NULL}, which cannot be done if the list is passed by value to a function.

\section{Exercise 4 (10 pts)}

We represent an arbitrarily large number by an unbounded array where each entry is $0$ or $1$, called the \emph{binary bits}. The digits $\beta_0, \ldots, \beta_{n-1}$ in an array of size $n$ represent the number

$$\sum\limits_{i=0}^{n-1} \beta_i 2^i.$$

\noindent The number zero is represented by an array of size $1$ and $\beta_0 = 0$. We want to implement a binary counter, that is, we want to support increments (=increase the number by 1) efficiently.

Aside:
Peano's representation $n ::= \texttt{O} \mid \texttt{S}\;n$ supports worst-case constant time increment and decrement.

\paragraph{a) Give pseudo-code for the increment operation. What is the worst-case running of a single increment?}

\paragraph{Answer:}
Ripple carry, where $\beta$ is an unbounded array of bits:
\begin{lstlisting}[xleftmargin=2cm]
inc($\beta$){
    n $\gets$ size($\beta$)
    i $\gets$ 0
    while $i < n \land \beta[i] = 1$ do
        $\beta[i] \gets 0$
        $i \gets i + 1$
    end do
    if $i < n$
    then $\beta[i] \gets 1$
    else push_back($\beta$,1)
    end if
}
\end{lstlisting}
Representing the natural number $m$ requires a bit array of size $\Theta(\log m)$.
In the worst case, the loop guard never sees a zero bit, so increment runs in $\Theta(\log m)$ worst-case time.

\paragraph{b) Show that starting from $0$, the cost of $m$ increments is $O(m)$.}

\begin{proof}
Attribution:
Based on course notes prepared by Jeff Erickson.\footnote{%
	The notes are at \url{http://web.engr.illinois.edu/~jeffe/teaching/algorithms/notes/15-amortize.pdf}.
	We stumbled on these notes during a rapid study of the fancy, parallel adders that hardware folks use.
	It hadn't crossed our minds to seriously consider ripple carry, or we might have attempted the amortized analysis without guidance.}

Let $E$ be a sequence of $m$ calls to increment, starting from zero and let $N$ be the total number of bits flipped during $E$.
The running time of $E$ is $\Theta(N)$.
(To see why, observe that for a given call to increment, we flip some bits and do $\Theta(1)$ other work.
We flip one bit (from one to zero) on each iteration of the loop and one additional bit (from zero to one) after the loop.
Each flip takes constant time.)
It thus suffices to show that $N$ is $O(m)$.

For $i \in I := \{0, \dots, \floor{\log_2 m}\}$, let $N_i$ be the number of times that bit $i$ is flipped during $E$.
We have $N = \sum_{i \in I} N_i$.
Bit 0 is flipped on every call, bit 1 on every other call, bit 2 on every fourth call, \ldots, bit $i$ on every $2^i$th call.
Thus
\[
	N = \sum_{i \in I} N_i = \sum_{i \in I} \floor{ \frac{m}{2^i} } \le m \sum_{i=0}^\infty \frac{1}{2^i} = 2m
\]
For the last step, observe that the sequence of partial sums $1$, $1\frac{1}{2}$, $1\frac{3}{4}$, $1\frac{7}{8}$, $\ldots$ has limit 2.
\end{proof}

\bigskip \noindent \textbf{c) Assume that we also support decrements of the counter. Argue why a sequence of $m$ increments/decrements can take up to $\Omega(m \log m)$ operations.}

\paragraph{Answer:}
Let $k \in \NN$ be given.
We can create the string $\alpha$ of $k$ ones using $a := 2^k -1$ increments in $O(a)$ time.
Incrementing $\alpha$ takes $\Theta(\log a)$ time and yields the string
\[
	\beta := 1 \overbrace{0 \cdots 0}^{k}
\]
comprising a one followed by $k$ zeros.
Decrementing $\beta$ (evident code omitted) takes $\Theta(\log a)$ time and yields $\alpha$.
Thus for any $r \in \NN$, a sequence of $2r$ calls, starting from $\alpha$ and switching between increment and decrement on each call, takes $\Theta(r \log a)$ time.
In particular, this holds when $r = a$.
Now recall exercise~(1c):
The sequence we've described runs in $O(a) + \Theta(a \log a) = \Omega(a \log a)$ time.

\bigskip

\section{Exercise 5 (10 pts)}

\declareops{alloc,realloc,size,init,pushback[push\any{}back],popback[pop\any{}back],get,set,copy}%
\declarevars{var,state,buf,cnt,from,to,cursor,N,D,H}%
\newcommand*{\SET}{\leftarrow}%
\declarestates{Normal,Halve,Double}%
\newcommand*{\pointsto}{\hookrightarrow}%
The problem has two parts:
\begin{itemize}
	\item[a)] Design a data structure for unbounded arrays supporting worst-case rather than amortized $\<pushback>$, $\<popback>$, $\<get>$, and $\<set>$ operations.
	
	\item[b)] Design a data structure for fixed-size arrays supporting constant-time $\<init>$, $\<get>$, and $\<set>$ operations.
	After $\<init>$, any item should have the value $\bot$.\footnote{%
		A better abstraction has $\<init>$ take an initial value; \ie $\<init>(n,v)$ creates an array of size $n$ with initial value $v$.
		This is better for \emph{clients} who no longer have to contend with $\bot$ unless they want to.
	}
\end{itemize}

\paragraph{Answer:}

We assume the following constant-time operations for fixed-size arrays:
\begin{itemize}
	\item $\<alloc>(n)$ allocates and returns a new array of size $n$ containing ``junk''
	\item $\<size>(A)$ returns the size of array $A$ (written $|A|$)
	\item $\<get>(A,i)$ returns the $i$th element of array $A$ when $0 \le i < |A|$ (written $A[i]$)
	\item $\<set>(A, i, v)$ sets the $i$th element of $A$ to $v$ when $0 \le i < |A|$ (written $A[i] \SET v$)
\end{itemize}
We can trivially implement the $\<size>$ operation atop more primitive arrays that lack it by storing, alongside a primitive array $\widehat A$, its size $n$.

\begin{itemize}

	\item[a)]
	The idea is to spread out the copying steps required to double or halve the size of our buffer amongst subsequent operations.
	When doubling, we keep our original array $A$ and an array $B$ with $|B| = 2|A|$.
	Each subsequent $\<pushback>$ does a unit of copying work.
	We take care in $\<popback>$ to \emph{abort} the copy if we pop the $|A|+1$st item.
	Halving is similar:
	We keep our original array $A$ and an array $B$ with $|B| = |A|/2$, $\<popback>$ does a unit of copying work, and $\<pushback>$ can abort.
	We keep our current state in $s$, which takes values in $\{\<N>, \<D>, \<H> \}$ and, when copying, a count $p$ of items remaining to copy.
	We give the code in \figref{fig:code}.

\begin{figure}
\begin{minipage}{0.32\linewidth}
\begin{lstlisting}
init(){
	$s \gets \<N>$
	$A \gets \<alloc>(1)$
	$n \gets 0$
	($B$, $p$ relevant when $s \not= \<N>$.)
}

get($i \in [0, n)$){
	if($i < |A|$) return $A[i]$
	else return $B[\,i - |A|\,]$
}

set($i \in [0, n)$, v){
	if($s = \<N>$) $A[i] \gets v$
	else if($s = \<H>$) $A[i] \gets B[i] \gets v$
	else{
		$B[i] \gets v$
		if($i < |A|$) $A[i] \gets v$
	}
}
\end{lstlisting}
\end{minipage}
\vrule~~
\begin{minipage}{0.32\linewidth}
\begin{lstlisting}
push_back($v$){
	if($s = \<N> \land n = |A|$){
		$s \gets \<D>$
		$B \gets \<alloc>(2|A|)$
		$B[n] \gets v$; $p \gets n$
	}
	else if($s = \<N>$) $A[n] = v$
	else if($s = \<D>$){
		$B[n] \gets v$
		copy()
	}
	else{
		$A[n] \gets B[n] \gets v$
		if($n = |B|/2$)
			$s \gets \<N>$
	}
	$n \gets n+1$
}
\end{lstlisting}
\end{minipage}
\vrule~~
\begin{minipage}{0.32\linewidth}
\begin{lstlisting}
pop_back(){
	if($s = \<N> \land n = |A|/4$){
		$s \gets \<H>$
		$B \gets \<alloc>(|A|/2)$
		$p \gets n$
	}
	else if($s = \<H>$) copy()
	else if($n = |A|$) $s \gets \<N>$
	$n \gets n-1$
}

copy(){
	$p \gets p - 1$
	$B[p] = A[p]$
	if($p = 0$){
		$s \gets \<N>$
		$A \gets B$
	}
}
\end{lstlisting}
\end{minipage}
\caption{Code for algorithm 5a.}
\label{fig:code}
\end{figure}

	\begin{figure}
	\begin{center}
	\begin{tikzpicture}[every node/.style=draw, node distance=2cm, thick]
		\node (N) {$\<N>$};
		\node (H) [below left of=N] {$\<H>$};
		\node (D) [below right of=N] {$\<D>$};
		\path[<->] (N) edge [bend left=45] (D);
		\path[<->] (N) edge [bend right=45] (H);
	\end{tikzpicture}
	\end{center}
	\caption{STS for algorithm 5(a).}
	\label{fig:sts}
	\end{figure}

	We specify our invariant with reference to the state-transition system in \figref{fig:sts}:
	\begin{itemize}
	
	\item $s = \<N>$:
	In this state, we proceed much as in the amortized data structure; \ie the fixed-size array $A$ stores $n$ items where
	\begin{mathpar}
		0 \le n < |A| \and
		\infer{i \in [0,n)}{\text{element $i$ at $A[i]$}}
	\end{mathpar}
	Let $w := |A|$.
	When inserting the $w+1$st item, we transition to state $\<D>$ ($\<pushback>$ lines 3--5).
	When removing the $w/4$th item, we transition to $\<H>$ ($\<popback>$ lines 3--5).
	
	\item $s = \<H>$:
	In this state, we incrementally copy the contents of the ``from-space'' $A$ to the ``to-space'' $B$, where $p$ says how many items remain to be copied and (as usual) $n$ says how many items we actually store for our client:
	\begin{mathpar}
		0 < p \le n < |B|/2 \and
		|B| = |A|/2 \and
		\infer{i \in [0, n)}{\text{element $i$ at $A[i]$}} \and
		\infer{i \in (p,n)}{\text{element $i$ at $B[i]$}} \and
	\end{mathpar}
	Line~7 in $\<popback>$ is the incremental continuation of the halving operation that was started in state $\<N>$.
	Note that $\<copy>$ transitions to state $\<N>$ when the copying job terminates.
	Line~15 in $\<pushback>$ aborts the copy if our client pushed us out of state $\<H>$.

	\item $s = \<D>$:
	This state is analogous to $\<H>$:
	\begin{mathpar}
		0 < p \le |A| \and
		|A| < n < |B| = 2|A| \\\\
		\infer{i \in [0, |A|)}{\text{element $i$ at $A[i]$}} \and
		\infer{i \in (p,|A|)}{\text{element $i$ at $B[i]$}} \and
		\infer{i \in [|A|,n)}{\text{element $i$ at $B[i]$}} \and
	\end{mathpar}
	Line~10 in $\<pushback>$ continues and line~8 in $\<popback>$ aborts the copying job.
	
	\end{itemize}
	Our boundary conditions match those of the amortized algorithm and we take care to abort copies that are rendered moot.
	This ensures that every doubling (halving) job terminates before the next must start.
	Obviously, every operation terminates in worst-case $\Theta(1)$ time.
	
	\item[b)]
	\declareops{newkey,mac}%
	Problem:
	We take the view that $\<alloc>$ adversarily chooses the contents of the memory it returns to us.
	Thus we cannot rely \emph{only} on relationships amongst values in the memory graph to decide if our client ``owns'' a particular array cell.

	Solution:
	We crypographically sign what we insert into the array:
	\begin{itemize}
	
		\item $\<init>(n)$ creates two arrays $V$ and $M$ of size $n$ and calls $\<newkey>$ to create a fresh key $k$.
		
		\item $\<set>(i,v)$ sets $V[i] \SET v$ and $M[i] \SET \<mac>(k,v)$.
		
		\item $\<get>(i)$ returns $V[i]$ if $M[i] = \<mac>(k, V[i])$; else $\bot$.
		
	\end{itemize}
	We use \emph{message authentication codes} and hidden state (a fresh key $k$ per array) to ensure that---up to standard computational assumptions for MACs---$\<set>$ returns the correct answer with overwhelming probability.
	(Of course, we assume a fixed bound on key and value sizes so that we may regard $\<mac>$ as operating in unit time.)
		
\end{itemize}

\end{document}
