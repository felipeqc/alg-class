\documentclass[a4paper]{article}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{times}
\usepackage{amssymb}
\usepackage{mathtools}	% pulls in amsmath
	\mathtoolsset{centercolon}
\usepackage{tikz}
	\usetikzlibrary{automata}
\usepackage{mathpartir}
\usepackage{amsthm}
\usepackage{amsxtra}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{semantic}
	\reservestyle{\declarevars}{\texttt}
	\reservestyle{\declareops}{\texttt}
	\reservestyle{\declarestates}{\text}
\usepackage{color}
\usepackage{listings}
\usepackage{mathtools}

\newtheorem{theorem}{Theorem}

\newtheorem{myexample}{\textbf{Example}}
\newtheorem{mylemma}{\textbf{Lemma}}
\newtheorem{myproof}{\textbf{Proof}}
\newtheorem{myinvariant}{\textbf{Invariant}}
\newtheorem{mytheorem}{\textbf{Theorem}}
\newtheorem{mycorollary}{\textbf{Corollary}}
\newtheorem{myapproach}{Approach}
\newtheorem{myproperty}{Property}
\newtheorem{mydefinition}{Definition}

\newtheorem{mycase}{Case}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
 % frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  morekeywords={then, end, and, or, assign, increment, decrement, jump, jump_if, store, *, +},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                       % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\newcommand*{\floor}[1]{\left\lfloor{#1}\right\rfloor}
\newcommand{\any}{{\rule[-.2ex]{1ex}{.4pt}}}	% Less hideous than \_.
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\RP}{\RR_{\ge 0}}
\newcommand*{\dave}[1]{{\color{red}\textbf{PDS: #1}}}
\newcommand{\ie}{\emph{i.e.,} }
\newcommand{\eg}{\emph{e.g.,} }
\usepackage{hyperref}
\newcommand*{\figref}[1]{\hyperref[#1]{Figure~\ref*{#1}}}

\title{Exercise Sheet 1---Algorithms and Data Structures}
\author{Felipe Cerqueira \\ 2547787 \and David Swasey \\ 2542105}

\begin{document}

\maketitle

Tutorial time: Monday 14:00

\section{Exercise 1 (10 pts)}

\paragraph{a)} Prove the statement from the lecture: If for two functions $f, g: \NN \to \RP$, the value $L := \lim_{n \to \infty} \frac{f(n)}{g(n)} \in [0, \infty)$ exists, then

\begin{itemize}
\item if $L = 0$, then $f(n) \in o(g(n))$.
\item if $L \in (0, \infty)$, then $f(n) \in \Theta(g(n))$.
\item if $L = \infty$, then $f(n) \in \omega(g(n))$.
\end{itemize}

\paragraph{Answer:}

We use the definition of limit.
(The definitions for O-notation assume that $f$ and $g$ are positive functions.
We modified the problem statement accordingly.)

\begin{itemize}
\item \textbf{If $L = 0$, then $f(n) \in o(g(n))$.}

By definition of limit at infinity:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0, \left | \frac{f(n)}{g(n)} - L \right | < c$$

Since $L=0$ and the functions are positive, this implies that:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0, f(n) \le c \cdot g(n)$$

Thus, $f(n) = o(g(n))$.

\bigskip

\item \textbf{if $L \in (0, \infty)$, then $f(n) \in \Theta(g(n))$.}

By definition of limit at infinity:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0, \left | \frac{f(n)}{g(n)} - L \right | < c$$

This is equivalent to:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0,  \left | \frac{f(n) - L \cdot g(n)}{g(n)} \right | < c$$

Solving the inequality with the absolute value produces two solutions. First:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0,  f(n) \le (c + L) \cdot g(n)$$


Therefore, $\exists c' = (c + L) > 0, \exists n_0 \in \mathbb{N}, \forall n \ge n_0$, $f(n) \le c' g(n)$.
That is, $f(n) = O(g(n))$.

\bigskip And as the second solution we have:

$$\forall c > 0, \exists n_0 \in \mathbb{N}, \forall n > n_0,  f(n) \ge (L - c) \cdot g(n)$$

Here $\exists c', 0 < c' < L, \exists n_0 \in \mathbb{N}, \forall n \ge n_0$, $f(n) \ge c' g(n)$. That is, $f(n) = \Omega(g(n))$.

\item \textbf{if $L = \infty$, then $f(n) \in \omega(g(n))$.}

By the definition of infinite limits at infinity,

$$\forall c > 0, \exists n_0>0, \forall n > n_0, \left | \frac{f(x)}{g(x)} \right | > c$$

That is,

$$\forall c > 0, \exists n_0>0, \forall n > n_0, f(x) > c \cdot g(x)$$

This is exactly the definition of $f(n) = \omega(g(n))$.


\end{itemize}

\paragraph{b)} Give an example of two (non-negative) functions $f,g$ with $f \in \Theta(g)$, but $\lim_{n \rightarrow \infty}\frac{f(n)}{g(n)}$ does not exist. 

\paragraph{Answer:}

Consider $f(n) = sin(n) + 1$ and $g(n) = cos(n) + 1$, which are both non-negative. Because the image of both functions lies in $[0, 2]$, they differ at most by some constant, so it must be that $f(n) = \Theta(g(n))$.
Now let's calculate the limit.

$$\lim_{n \rightarrow \infty} \frac{\sin(n) + 1}{\cos(n) + 1} = 
\lim_{n \rightarrow \infty} \frac{\sin(n)}{\cos(n) + 1} + \lim_{n \rightarrow \infty} \frac{1}{\cos(n) + 1}$$

By L'Hospital's Rule:

 
$$= \lim_{n \rightarrow \infty} \left ( \frac{\cos(n)}{-\sin(n)} + \frac{1}{-\sin(n)} \right )= \lim_{n \rightarrow \infty} \left (- \cot (n) - \csc (n) \right )$$

But since this function is periodic, the limit at infinity does not exist.

\paragraph{c)} Prove the following rules for O-notation:
\paragraph{Answer:}
\begin{itemize}
\item \textbf{For any positive constant} $c$, $c \cdot f(n) = \Theta(f(n))$.

Let $c$ be any positive constant. Note that $\forall n, c f(n) \le c f(n)$. Then $\exists c' = c > 0$ and $\exists n_0 = 0$, such that $\forall n \ge n_0$, $c\cdot f(n) \le c' \cdot f(n)$. So $c f(n) = O(f(n))$.

Likewise, $\forall n, c f(n) \ge c f(n)$. Then $\exists c' = c > 0$ and $\exists n_0 = 0$, such that $\forall n \ge n_0$, $c\cdot f(n) \ge c' \cdot f(n)$. So $c f(n) = \Omega(f(n))$.

\item $\mathbf{f(n) + g(n) = \Omega(f(n))}$.

Since $g(n) \ge 0$, then $f(n) + g(n) \ge 1 \cdot f(n)$. Thus, $\exists c = 1 > 0$ and $\exists n_0 = 0$, such that $\forall n \ge n_0, g(n) + f(n) \ge c \cdot f(n)$.

\item If $\mathbf{g(n) = O(f(n))}$, then $\mathbf{f(n) + g(n) = O(f(n))}$.

Assume that $\exists c > 0$ and $\exists n_0 \in 
\mathbb{N}$, such that $\forall n \ge n_0$, $g(n) \le c \cdot f(n)$.
Then, $\forall n \ge n_0$, $f(n) + g(n) \le f(n) + c \cdot f(n) = (c+1) \cdot f(n)$.

Therefore, $\exists c' = (c+1)$ and $\exists n_0 \in 
\mathbb{N}$, such that $\forall n \ge n_0$, $f(n) + g(n) \le c' \cdot f(n)$. That is, $f(n) + g(n) = O(f(n))$.


\item $O(f(n)) \cdot O(g(n)) = O(f(n) \cdot g(n))$.

Consider any $y(n) = O(f(n))$ and any $y' = O(g(n))$.

We know that $\exists c > 0$ and $\exists n_0 \in 
\mathbb{N}$, such that $\forall n \ge n_0$, $y(n) \le c \cdot f(n)$.

Similarly, $\exists c' > 0$ and $\exists n_0' \in 
\mathbb{N}$, such that $\forall n \ge n_0'$, $y'(n) \le c' \cdot g(n)$.

Let $n_0'' = n_0 + n_0'$.

Then $\forall n \ge n_0''$,

$$y(n) \le c \cdot f(n)$$

$$y'(n) \le c' \cdot g(n)$$

That is,

$$y(n) \cdot y'(n) \le c \cdot c' \cdot f(n) \cdot g(n)$$

Therefore, $\exists c'' = c \cdot c' > 0$ and $\exists n_0'' \in 
\mathbb{N}$, such that $\forall n \ge n_0''$, $y(n)\cdot y'(n) \le c'' \cdot f(n) \cdot g(n)$. It follows that $O(f(n)) \cdot O(g(n)) = O(f(n) \cdot g(n))$.

\end{itemize}

\paragraph{d)} Show that $a^n = o(b^n)$ and $\log_a n = \Theta (\log_b n)$ for any $1 < a < b$.

\paragraph{Answer:}

For the first case, note that $\lim_{n \rightarrow \infty}\frac{a^n}{b^n} = \lim_{n \rightarrow \infty}\left (\frac{a}{b} \right )^n = 0$, since $a/b < 1$. Thus, by the theorem from the lecture, $a^n = o(b^n)$.

For the other function:

$$\lim_{n \rightarrow \infty}\frac{\log_a n}{\log_b n} = \lim_{n \rightarrow \infty}\frac{\log_b n}{\log_b a \cdot \log_b n} = \lim_{n \rightarrow \infty} \log_a b \in (0,\infty)$$

By the theorem from the lecture, $\log_a n = \Theta(\log_b n)$.

\section{Exercise 2 (10 pts)}
\newcommand{\prob}{\operatorname{prob}}%

\paragraph{a)} Let $I$ and $J$ be two independent indicator variables with $\prob(X = 1) = p$ and $\prob(Y = 1) = q$, $p, q \in [0,1]$. Let $X = ( I + J) \mod 2$, that is, $X$ is one if and only if $I$ and $J$ are different. For which values of $p$ and $q$ are $I$ and $X$ independent? For which values are $I$, $J$, and $X$ independent?

\paragraph{Answer:}

Since $I$ and $J$ are independent, we can make a table with the events and probabilities.

\begin{center}
\begin{tabular}{ c | c | c | c }
  I = i & J = j & X = x & $\prob(I = i \wedge J = j \wedge X = x)$\\
  \hline                       
  0 & 0 & 0 & $(1-p)\cdot (1-q)$\\
  0 & 1 & 1 & $(1-p)\cdot q$\\
  1 & 0 & 1 & $p\cdot (1-q)$\\
  1 & 1 & 0 & $p \cdot q$\\
  \hline  
\end{tabular}
\end{center}

To enforce independence of $I$ and $X$, we must guarantee that:

$$ \forall i, x \in \{0,1\}, \prob(I = i \wedge X = x) = \prob(I = i) \cdot \prob(X = 1) $$
Expanding the quantifiers:
\begin{align*}
\prob(I = 0 \wedge X = 0) &= \prob(I = 0) \cdot \prob(X = 0) \land{} \\
\prob(I = 0 \wedge X = 1) &= \prob(I = 0) \cdot \prob(X = 1) \land{} \\
\prob(I = 1 \wedge X = 0) &= \prob(I = 1) \cdot \prob(X = 0) \land{} \\
\prob(I = 1 \wedge X = 1) &= \prob(I = 1) \cdot \prob(X = 1)
\end{align*}

Solving this set of equations, we obtain $p = 0$ and $q = 1/2$.

\bigskip
To enforce independence of $I$, $J$ and $X$, we must guarantee that:

$$ \forall i, j, x \in \{0,1\}, \prob(I = i \wedge J = j \wedge X = x) = \prob(I = i) \cdot \prob(J = j) \cdot \prob(X = 1) $$

However, this include the constraints for the impossible events:
\begin{align*}
0 = \prob(I = 0 \wedge J = 0 \wedge X = 1) = \prob(I = 0) \cdot \prob(J = 0) \cdot \prob(X = 1) \\
0 = \prob(I = 0 \wedge J = 1 \wedge X = 0) = \prob(I = 0) \cdot \prob(J = 1) \cdot \prob(X = 0) \\
0 = \prob(I = 1 \wedge J = 0 \wedge X = 0) = \prob(I = 1) \cdot \prob(J = 0) \cdot \prob(X = 0) \\
0 = \prob(I = 1 \wedge J = 1 \wedge X = 1) = \prob(I = 1) \cdot \prob(J = 1) \cdot \prob(X = 1)
\end{align*}

Because we know that each of the three variables can assume all values in $\{0, 1\}$ (\ie, $\prob(I = i), \prob(J = j), \prob(X = 1) > 0$), these equations cannot hold. Therefore, $I$, $J$, and $X$ cannot be made independent simultaneously.

\paragraph{b)} Let $X_1, \ldots, X_n$ be independent indicator variables with $\prob(X_i = 1) = p$ for all $i$. Show that $X = X_1 + \cdots + X_n$ is \emph{binomially distributed}, that is

$$\prob(X = k) = {n \choose k} p^k (1-p)^{n-k}.$$

\paragraph{Answer:}

\begin{proof}
By induction on the number of variables $n$. Let $X^{(i)} = X_1 + \cdots + X_i$. For the base case with a single variable $X_1$, we have:

\begin{align*}
\prob(X^{(1)} = 0) = \prob(X_1 = 0) = (1 - p) = {1 \choose 0} p^0 (1-p)^{1} \\
\prob(X^{(1)} = 1) = \prob(X_1 = 1) = p = {1 \choose 1} p^1 (1-p)^{0}
\end{align*}

For our IH, assume that for $i<n$ and $X^{(i)} = X_1 + \cdots + X_i$, the following holds:

$$\prob(X^{(i)} = k) = {i \choose k} p^k (1-p)^{i-k}.$$

Let's now consider the variable $X^{(i+1)} = X^{(i)} + X_{i+1}$, which includes one more term in the sum. Because $X_{i+1}$ can only assume two mutually-exclusive values (0 or 1):

\begin{align*}
\prob(X^{(i+1)} = k) & = \prob(X^{(i)} = k-1 \wedge X_{i+1} = 1) + \prob(X^{(i)} = k \wedge X_{i+1} = 0) && \text{[$X$ and $X_{i+1}$ are independent]}\\
& = \prob(X^{(i)} = k-1) \cdot \prob(X_{i+1} = 1) + \prob(X^{(i)} = k) \cdot \prob(X_{i+1} = 0) && \text{[Initial assumption]} \\
& = \prob(X^{(i)} = k-1) \cdot p + \prob(X^{(i)} = k) \cdot (1 - p) && \text{[By IH]}\\
& = {i \choose k-1} p^{k-1} (1-p)^{i-k+1} \cdot p + {i \choose k} p^k (1-p)^{i-k} \cdot (1 - p)\\
& = {i \choose k-1} p^{k} (1-p)^{i-k+1} + {i \choose k} p^k (1-p)^{i-k+1} \\
& = \left [ {i \choose k-1} + {i \choose k} \right] p^{k} (1-p)^{i-k+1}
&& \text{[By Pascal's rule]} \\
& =  {{i+1} \choose k} p^{k} (1-p)^{i-k+1}
\end{align*}

Therefore, the binomial property also holds when we include the extra term $X_{i+1}$. This completes the induction proof.
\end{proof}


\section{Exercise 3 (10 pts)}

Consider the following two alternatives to the list data structures as discussed in the lecture:

a) Every item has a pointer to its successor, but not its predecessor (singly-linked lists)

b) The list is doubly connected, but does not start with a dummy item. Instead, the head
pointer of the list points to the first element. Its predecessor and the successor of the last list element point to a special value \emph{NULL}.

\bigskip Discuss the advantages and disadvantages of these implementations, both in terms of asymptotic complexity and practical considerations.

\paragraph{Answer:}

Implementation (a) has the advantage of using less memory for the representation (only half the number of pointers).
Also, the operations are less error-prone as there are fewer pointer manipulations.
Operations at the head of the list continue to take constant time (\eg testing if the list is empty, consing on a new element, or removing the list head).
However, it makes some operations on the list more time-consuming (\eg adding an element at the end of the list, or concatenating two lists).
In general, any move/delete operation on some element $e$ requires updating the ``next-pointer'' from the element $e'$ that precedes $e$.
Therefore, these operations always need to be started from some predecessor of $e$, in order to identify this element $e'$.

Regarding implementation (b), there are no major drawbacks, except that it makes the list less modular and more susceptible to low-level programming issues. For example, deleting every element would require setting the $head$ pointer to \emph{NULL}, which cannot be done if the list is passed by value to a function.

\section{Exercise 4 (10 pts)}

We represent an arbitrarily large number by an unbounded array where each entry is $0$ or $1$, called the \emph{binary bits}. The digits $\beta_0, \ldots, \beta_{n-1}$ in an array of size $n$ represent the number

$$\sum\limits_{i=0}^{n-1} \beta_i 2^i.$$

\noindent The number zero is represented by an array of size $1$ and $\beta_0 = 0$. We want to implement a binary counter, that is, we want to support increments (=increase the number by 1) efficiently.

\noindent \textbf{a) Give pseudo-code for the increment operation. What is the worst-case running of a single increment?}

\paragraph{Answer:}
Ripple carry, where $\beta$ is an unbounded array of bits:
\begin{lstlisting}[mathescape,xleftmargin=2cm]
inc($\beta$){
    n $\gets$ size($\beta$)
    i $\gets$ 0
    while $i < n \land \beta[i] = 1$ do
        $\beta[i] \gets 0$
        $i \gets i + 1$
    end do
    if $i < n$
    then $\beta[i] \gets 1$
    else push_back($\beta$,1)
    end if
}
\end{lstlisting}
Representing the natural number $m$ requires a bit array of size $\Theta(\log m)$.
In the worst case, the loop guard never sees a zero bit, so increment runs in $\Theta(\log m)$ worst-case time.

\bigskip \noindent \textbf{b) Show that starting from $0$, the cost of $m$ increments is $O(m)$.}

\begin{proof}
Attribution:
Based on course notes prepared by Jeff Erickson.\footnote{%
	The notes are at \url{http://web.engr.illinois.edu/~jeffe/teaching/algorithms/notes/15-amortize.pdf}.
	We stumbled on these notes during a rapid study of the fancy, parallel adders that hardware folks use.
	It hadn't crossed our minds to seriously consider ripple carry, or we might have attempted the amortized analysis without guidance.}

Let $E$ be a sequence of $m$ calls to increment, starting from zero and let $N$ be the total number of bits flipped during $E$.
The running time of $E$ is $\Theta(N)$.
(To see why, observe that for a given call to increment, we flip some bits and do $\Theta(1)$ extra work.
We flip one bit (from one to zero) on each iteration of the loop and one additional bit (from zero to one) after the loop.
Each flip takes constant time.)
It thus suffices to show that $N$ is $O(m)$.

For $i \in I := \{0, \dots, \floor{\log_2 m}\}$, let $N_i$ be the number of times that bit $i$ is flipped during $E$.
We have $N = \sum_{i \in I} N_i$.
Bit 0 is flipped on every call, bit 1 on every other call, bit 2 on every fourth call, \ldots, bit $i$ on every $2^i$th call.
Thus
\[
	N = \sum_{i \in I} N_i = \sum_{i \in I} \floor{ \frac{m}{2^i} } \le m \sum_{i=0}^\infty \frac{1}{2^i} = 2m
\]
For the last step, observe that the sequence of partial sums $1$, $1\frac{1}{2}$, $1\frac{3}{4}$, $1\frac{7}{8}$, $\ldots$ has limit 2.
\end{proof}

\bigskip \noindent \textbf{c) Assume that we also support decrements of the counter. Argue why a sequence of $m$ increments/decrements can take up to $\Omega(m \log m)$ operations.}

\paragraph{Answer:}
Let $k \in \NN$ be given.
We can create the string $\alpha$ of $k$ ones using $a := 2^k -1$ increments in $O(a)$ time.
Incrementing $\alpha$ takes $\Theta(\log a)$ time and yields the string
\[
	\beta := 1 \overbrace{0 \cdots 0}^{k}
\]
comprising a one followed by $k$ zeros.
Decrementing $\beta$ (evident code omitted) takes $\Theta(\log a)$ time and yields $\alpha$.
Thus for any $r \in \NN$, a sequence of $2r$ calls, starting from $\alpha$ and switching between increment and decrement on each call, takes $\Theta(r \log a)$ time.
In particular, this holds when $r = a$.
Now recall exercise~(1c):
The sequence we've described runs in $O(a) + \Theta(a \log a) = \Omega(a \log a)$ time.

\bigskip

\section{Exercise 5 (10 pts)}

\declareops{alloc,realloc,size,init,pushback[push\any{}back],popback[pop\any{}back],get,set,copy}%
\declarevars{var,state,buf,cnt,from,to,cursor}%
\newcommand*{\SET}{\leftarrow}%
\declarestates{Normal,Halve,Double}%
\newcommand*{\pointsto}{\hookrightarrow}%
\newcommand{\stsfig}{%
	\begin{figure}
	\begin{center}
	\begin{tikzpicture}[every node/.style=draw, node distance=3cm, thick]
		\node (N) {$\<Normal>(A, n)$};
		\node (H) [below left of=N] {$\<Halve>(A, B, c, n)$};
		\node (D) [below right of=N] {$\<Double>(A, B, c, n)$};
		\path[<->] (N) edge [bend left=45] (D);
		\path[<->] (N) edge [bend right=45] (H);
	\end{tikzpicture}
	\end{center}
	\caption{STS for algorithm 5(a).}
	\label{fig:sts}
	\end{figure}
}%
\newcommand{\codefig}{%
	\begin{figure}\centering
	\begin{minipage}{0.49\linewidth}
	\begin{algorithmic}[1]
		\Require $\<Halve>(A,B,c,n)$ and $n > 0$
		\Ensure $\<Halve>(A,B,c+1,n-1)$ or $\<Normal>(B,n-1)$
		\Procedure{pop\any{}back}{}
			\State $\<cnt> \SET \<cnt>-1$\label{Hpop}
			\State{$\<to>[\<cursor>] \SET \<from>[\<cursor>]$}
			\If{$\<cursor>+1 < \<cnt>$}
				\State{$\<cursor> \SET \<cursor>+1$}
			\Else
				\State $\<state> \SET \<Normal>$ \label{HtoN}
				\State $\<buf> \SET \<to>$
			\EndIf
		\EndProcedure
		\Statex
		\Require $\<Halve>(A,B,c,n)$
		\Ensure $\<Halve>(A,B,c,n+1)$ or $\<Normal>(A,n+1)$
		\Procedure{push\any{}back}{$v$}
			\State $\<cnt> \SET \<cnt>+1$
			\State $\<from>[\<cnt>] \SET v$
			\If{$\<cnt> = |\<to>|/2$}
				\State $\<state> \SET \<Normal>$
				\State $\<buf> \SET \<from>$
			\EndIf
		\EndProcedure
	\end{algorithmic}
	\end{minipage}
	\caption{Pseudocode for the interesting operations in state $\protect\<Halve>(A,B,c,n)$.}
	\label{fig:halveops}
	\end{figure}
}%
The problem has two parts:
\begin{itemize}
	\item[a)] Design a data structure for unbounded arrays supporting worst-case rather than amortized $\<pushback>$, $\<popback>$, $\<get>$, and $\<set>$ operations.
	
	\item[b)] Design a data structure for fixed-size arrays supporting constant-time $\<init>$, $\<get>$, and $\<set>$ operations.
	After $\<init>$, any item should have the value $\bot$.\footnote{%
		A better abstraction has $\<init>$ take an initial value; \ie $\<init>(n,v)$ creates an array of size $n$ with initial value $v$.
		This is better for \emph{clients} who no longer have to contend with $\bot$ unless they want to.
	}
\end{itemize}

\paragraph{Answer:}

We assume the following constant-time operations for fixed-size arrays:
\begin{itemize}
	\item $\<alloc>(n)$ allocates and returns a new array of size $n$ containing ``junk''
	\item $\<size>(A)$ returns the size of array $A$ (written $|A|$)
	\item $\<get>(A,i)$ returns the $i$th element of array $A$ when $0 \le i < |A|$ (written $A[i]$)
	\item $\<set>(A, i, v)$ sets the $i$th element of $A$ to $v$ when $0 \le i < |A|$ (written $A[i] \SET v$)
\end{itemize}
We can trivially implement the $\<size>$ operation atop more primitive arrays that lack it by storing, alongside a primitive array $\widehat A$, its size $n$.

\begin{itemize}

	\item[a)]
	One can \emph{incrementalize} amortized data structures to obtain worst-case data structures~\cite{okasaki}.
	That this is possible should come as no surprise:
	To prove amortized time bounds, we \emph{logically} schedule the work of expensive operations amongst cheaper operations.
	With additional bookkeeping, we may \emph{phsically} do so, provided the bookkeeping code does not upset our complexity goals.
	The amortized analysis shows not only that such scheduling is possible, but that the incremental work will be complete before their results are needed.
	
	In this case, the amortized algorithm stores $n$ items in a capacity $w \ge n$ fixed-sized array, doubling $w$ when pusing the $w+1$st item and halving $w$ when popping the $w/4$th item.
	The doubling and halving steps allocate a new array, copy the $n$ items to it, then switch to using it.
	We seek to incrementalize these $\Theta(n)$ copy operations.
	
	\stsfig

	One way to describe our data structure is via the state-transition system in \figref{fig:sts}.
	An invariant \emph{interprets} each state by specifying what our data stucture must look like in that state.
	Our operations may rely on the invariant when they start and must guarantee it when they terminate.

	\begin{itemize}
	
	\item $\<Normal>(A, n)$:
	In this state, we proceed much as in the amortized data structure; \ie the fixed-size array $A$ stores $n$ items where $0 \le n < |A| =: w$.
	Our interpretation requires\footnote{%
		We systematically distinguish (mathematical) variables (\eg $A$, $n$) from the ``variables'' comprising our algorithm's state (\eg $\<buf>$, $\<cnt>$).
		The assertion $\<var> \pointsto v$ connects the two, saying that the location $\<var>$ points to the value $v$.
	}
	\begin{mathpar}
		\<state>\pointsto \<Normal> \and
		\<buf> \pointsto A \and
		\<cnt> \pointsto n \\\\
		0 \le n \le |A| \and
		\infer{i \in [0,n)}{\text{element $i$ at $A[i]$}}
	\end{mathpar}
	When inserting the $w+1$st item, we transition to $\<Double>(A,B,0,n)$ where $B$ is a fresh array allocated with $\<alloc>$ and $|B| = 2w$ and $n = w+1$.
	When removing the $w/4$th item, we transition to $\<Halve>(A,B,0,n)$, where $|B| = w/2$ and $n = w/4 - 1$.
	
	We omit the code for this state.
	
	\item $\<Halve>(A, B, c, n)$:
	In this state, we incrementally copy the relevant contents of the ``from-space'' $A$ to the ``to-space'' $B$, where the ``cursor'' $c$ says how many items we have copied and (as usual) $n$ says how many items we actually store for our client.
	We interpret this state as follows.
	\begin{mathpar}
		\<state>\pointsto \<Halve> \and
		\<from> \pointsto A \and
		\<to> \pointsto B \and
		\<cursor> \pointsto c \and
		\<cnt> \pointsto n \\\\
		0 \le c < n < |B|/2 \and
		|B| = |A|/2 \and
		\infer{i \in [0,c)}{\text{element $i$ at $A[i]$ and $B[i]$}} \and
		\infer{i \in [c, n)}{\text{element $i$ at $A[i]$}}
	\end{mathpar}
	We give pseudocode for this state's $\<pushback>$ and $\<popback>$ operations in \figref{fig:halveops}.

	The $\<popback>$ operation decrements $\<cnt>$ and performs a unit of copying work, incrementing $\<cursor>$ and transitioning to the normal state when we're done copying.
	Note that we \emph{must} transition in line~\ref{HtoN}; otherwise, we would fail to satisfy our invariant.
	
	The $\<pushback>$ operation updates $\<from>$ and increments $\<cnt>$.
	If the new value of $\<cnt>$ disagrees with our interpretation, then we \emph{abort} the on-going copy.
	This sets us up for a later transition to $\<Double>$, in case our client continues to push values.

	\item $\<Double>(A, B, c, n)$:
	This state is analogous to $\<Halve>(A, B, c, n)$, but with interpretation:
	\begin{mathpar}
		\<state>\pointsto \<Double> \and
		\<from> \pointsto A \and
		\<to> \pointsto B \and
		\<cursor> \pointsto c \and
		\<cnt> \pointsto n \\\\
		0 \le c < |A| \and
		|A| < n < |B| = 2|A| \\\\
		\infer{i \in [0,c)}{\text{element $i$ at $A[i]$ and $B[i]$}} \and
		\infer{i \in [c, |A|)}{\text{element $i$ at $A[i]$}} \and
		\infer{i \in [|A|,n)}{\text{element $i$ at $B[i]$}}
	\end{mathpar}
	We omit the corresponding code.
	\end{itemize}
	Our interpretation tells us, among other things, how to access the $i$th item in our data structure and thus how to properly implement the $\<get>$ and $\<set>$ operations for each state.
	Concretely, all our operations begin with a case analysis on the current contents of $\<state>$, branching to state-specific code.

	\codefig
	
	\item[b)]
	\declareops{newkey,mac}%
	Problem:
	We take the view that $\<alloc>$ adversarily chooses the contents of the memory it returns to us.
	Thus we cannot rely \emph{only} on relationships amongst values in the memory graph to decide if our client ``owns'' a particular array cell.

	Solution:
	We crypographically sign what we insert into the array:
	\begin{itemize}
	
		\item $\<init>(n)$ creates two arrays $V$ and $M$ of size $n$ and calls $\<newkey>$ to create a fresh key $k$.
		
		\item $\<set>(i,v)$ sets $V[i] \SET v$ and $M[i] \SET \<mac>(k,v)$.
		
		\item $\<get>(i)$ returns $V[i]$ if $M[i] = \<mac>(k, V[i])$; else $\bot$.
		
	\end{itemize}
	We use \emph{message authentication codes} and hidden state (a fresh key $k$ per array) to ensure that---up to standard computational assumptions for MACs---$\<set>$ returns the correct answer with overwhelming probability.
	(Of course, we assume a fixed bound on key and value sizes so that we may regard $\<mac>$ as operating in unit time.)
		
\end{itemize}

\end{document}
